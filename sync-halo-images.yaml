AWSTemplateFormatVersion: "2010-09-09"
Description: Creates a serverless application which executes a DataSync on a schedule and sends notifications on success or failure.

Parameters:
  TaskName:
    Description: Task name can include letters (A-Z and a-z), numbers (0-9), and dashes (-).
    Type: String
    Default: HaloImages
  SourceLocationId: 
    Description: DataSync source location ID
    Type: String
  DestinationLocationId: 
    Description: DataSync destination location ID
    Type: String
  NotificationEmail:
    Description: Email address to receive notifications (usually a mailing list)
    Type: String
  TaskSchedule:
    Description: A cron expression (in UTC time) which specifies when this task should be run.
    Type: String
    Default: "cron(0 23 * * ? *)"
  DestinationBucket:
    Description: Destination S3 bucket
    Type: String
  FileOwner:
    Description: SMB file owner
    Type: String
  FileGroup:
    Description: SMB file group
    Type: String
  FilePermissions:
    Description: SMB file permissions
    Type: String
  FileAcl:
    Description: SMB file ACL
    Type: String
  SyncAllFiles:
    Description: Sync all files in the source location
    Type: String
    AllowedValues: ["true", "false"]
    Default: "false"

Metadata: 
  AWS::CloudFormation::Interface: 
    ParameterGroups:
      - Label: 
          default: Task Name
        Parameters: 
          - TaskName
      - Label: 
          default: Locations
        Parameters: 
          - SourceLocationId
          - DestinationLocationId
      - Label: 
          default: Configuration
        Parameters: 
          - TaskSchedule
          - SyncAllFiles
      - Label: 
          default: Notifications
        Parameters: 
          - NotificationEmail
      - Label: 
          default: S3 Destination Metadata
        Parameters: 
          - DestinationBucket
          - FileOwner
          - FileGroup
          - FilePermissions
          - FileAcl

Resources:

  ##########################
  # Create a DataSync task #
  ##########################

  # Creates a CloudWatch log group for the DataSync task which retains entries for 180 days
  DataSyncTaskLogGroup:
    Type: "AWS::Logs::LogGroup"
    Properties: 
      LogGroupName: !Sub "${TaskName}DataSyncTask"
      RetentionInDays: 180

  # Creates a CloudWatch Logs resource policy which allows DataSync to create log streams and to put log events for the specified log group
  DataSyncTaskLogsResourcePolicy:
    Type: AWS::Logs::ResourcePolicy
    Properties:
      PolicyName: !Sub "${TaskName}DataSyncTaskLogsResourcePolicy" 
      PolicyDocument: !Sub "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"${TaskName}DataSyncTaskLogsResourcePolicy\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": [ \"datasync.amazonaws.com\" ] }, \"Action\": [ \"logs:PutLogEvents\", \"logs:CreateLogStream\" ], \"Resource\": \"${DataSyncTaskLogGroup.Arn}\" } ] }"
    DependsOn: DataSyncTaskLogGroup

  # Creates a DataSync task from the provided source and destination
  DataSyncTask:
    Type: "AWS::DataSync::Task"
    Properties:
      Name: !Ref TaskName
      # CloudWatchLogGroupArn: !GetAtt DataSyncTaskLogGroup.Arn # wildcard is not supported
      CloudWatchLogGroupArn: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${DataSyncTaskLogGroup}"
      SourceLocationArn: !Sub "arn:aws:datasync:${AWS::Region}:${AWS::AccountId}:location/${SourceLocationId}"
      DestinationLocationArn: !Sub "arn:aws:datasync:${AWS::Region}:${AWS::AccountId}:location/${DestinationLocationId}"
      Options: 
          LogLevel: TRANSFER
          OverwriteMode: NEVER
          PreserveDeletedFiles: PRESERVE
          TaskQueueing: ENABLED
          TransferMode: CHANGED
          VerifyMode: ONLY_FILES_TRANSFERRED
    DependsOn: DataSyncTaskLogGroup


  #############################################################
  # Create a Lambda function which executes the DataSync task #
  #############################################################

  # Creates a CloudWatch log group for a Lambda function which executes the DataSync task
  DataSyncTaskExecutionLogGroup:
    Type: "AWS::Logs::LogGroup"
    Properties: 
      LogGroupName: !Sub "/aws/lambda/${TaskName}DataSyncTask"
      RetentionInDays: 180

  # Creates an IAM role for a Lambda function which executes the DataSync task
  DataSyncTaskExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal: 
              Service:
                - lambda.amazonaws.com
            Action:
               "sts:AssumeRole"
      Policies:
        - PolicyName: !Sub "${TaskName}DataSyncTaskExecutionPolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: 
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: !GetAtt DataSyncTaskExecutionLogGroup.Arn
              - Effect: Allow
                Action: 
                  - "datasync:StartTaskExecution"
                Resource: !GetAtt DataSyncTask.TaskArn
    DependsOn: 
      - DataSyncTask
      - DataSyncTaskExecutionLogGroup

  # Creates a Lambda function which executes the DataSync task
  DataSyncTaskExecutionLambda:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: !Sub "${TaskName}DataSyncTask"
      Description: !Sub "${TaskName} - Executes the DataSync task" 
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 900
      Runtime: python3.9
      Role: !GetAtt DataSyncTaskExecutionRole.Arn
      Environment:
        Variables:
          TZ: America/New_York
          TaskArn: !GetAtt DataSyncTask.TaskArn
          SyncAllFiles: !Ref SyncAllFiles
      Code: 
        ZipFile: |
          import json
          from datetime import date, timedelta
          from os import getenv
          from boto3 import client

          def lambda_handler(event, context):
              # assume folder name is YYYY-MM-DD
              yesterday = date.today() - timedelta(days=1)
              sync_folder = "/" + yesterday.strftime("%Y-%m-%d")

              print(getenv("TaskArn"))
              print(sync_folder)

              # execute task and return task execution ARN
              if getenv("SyncAllFiles") == "true":
                  return client("datasync").start_task_execution(
                      TaskArn=getenv("TaskArn"),
                      OverrideOptions={"TransferMode": "ALL"}
                  )
              else:
                  return client("datasync").start_task_execution(
                    TaskArn=getenv("TaskArn"),
                    Includes=[{"FilterType": "SIMPLE_PATTERN", "Value": sync_folder}],
                    OverrideOptions={"TransferMode": "ALL"}
                  )
    DependsOn: 
      - DataSyncTask
      - DataSyncTaskExecutionRole

  # Creates an EventBridge Rule which executes the DataSync task on a daily basis (at 23:00)
  DataSyncTaskExecutionEventRule:
    Type: "AWS::Events::Rule"
    Properties:
      Name: !Sub "${TaskName}DataSyncTaskExecution"
      Description: !Sub "${TaskName}DataSyncTask - Executes the specified Lambda function on a daily basis"
      Targets:
        - Id: !Ref DataSyncTaskExecutionLambda
          Arn: !GetAtt DataSyncTaskExecutionLambda.Arn
      ScheduleExpression: !Ref TaskSchedule
    DependsOn: 
      - DataSyncTaskExecutionLambda
  
  # Grants permission for the DataSyncTaskExecutionEventRule to invoke the DataSyncTaskExecutionLambda function
  DataSyncTaskExecutionLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      Action: "lambda:InvokeFunction"
      FunctionName: !GetAtt DataSyncTaskExecutionLambda.Arn
      Principal: events.amazonaws.com
      SourceArn: !GetAtt DataSyncTaskExecutionEventRule.Arn
    DependsOn:
      - DataSyncTaskExecutionEventRule
      - DataSyncTaskExecutionLambda


  ##################################################################################################
  # Create an SNS Topic and a Lambda function which publishes status updates for the DataSync task #
  ##################################################################################################

  # Creates a SNS topic and a subscription for receiving updates about DataSync task execution status
  DataSyncTaskNotificationsTopic:
    Type: "AWS::SNS::Topic"
    Properties:
      TopicName: !Sub "${TaskName}DataSyncNotifications"
      Subscription:
        - Protocol: email
          Endpoint: !Ref NotificationEmail

  # Creates a CloudWatch log group for a Lambda function which publishes updates about DataSync task execution status
  DataSyncTaskNotificationsLogGroup:
    Type: "AWS::Logs::LogGroup"
    Properties: 
      LogGroupName: !Sub "/aws/lambda/${TaskName}DataSyncTaskNotifications"
      RetentionInDays: 180

  # Creates an IAM role for a Lambda function which publishes updates about DataSync task execution status
  DataSyncTaskNotificationsRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal: 
              Service:
                - lambda.amazonaws.com
            Action:
               "sts:AssumeRole"
      Policies:
        - PolicyName: !Sub "${TaskName}DataSyncTaskNotificationsPolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: 
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: !GetAtt DataSyncTaskNotificationsLogGroup.Arn
              - Effect: Allow
                Action: 
                  - "logs:DescribeLogStreams"
                  - "logs:GetLogEvents"
                Resource: !GetAtt DataSyncTaskLogGroup.Arn
              - Effect: Allow
                Action: 
                  - "datasync:DescribeTask"
                Resource: !GetAtt DataSyncTask.TaskArn
              - Effect: Allow
                Action: 
                  - "datasync:DescribeTaskExecution"
                Resource: !Sub "${DataSyncTask.TaskArn}/execution/*"
              - Effect: Allow
                Action:
                  - "sns:Publish"
                Resource: !Ref DataSyncTaskNotificationsTopic
    DependsOn: 
      - DataSyncTask
      - DataSyncTaskLogGroup
      - DataSyncTaskNotificationsLogGroup

  # Creates a Lambda function which publishes updates about DataSync task execution status
  DataSyncTaskNotificationsLambda:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: !Sub "${TaskName}DataSyncTaskNotifications"
      Description: !Sub "${TaskName} - Publishes updates about DataSync task execution status" 
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 900
      Runtime: python3.9
      Role: !GetAtt DataSyncTaskNotificationsRole.Arn
      Environment:
        Variables:
          TZ: America/New_York
          TaskName: !Ref TaskName
          TaskArn: !GetAtt DataSyncTask.TaskArn
          LogGroupArn: !GetAtt DataSyncTaskLogGroup.Arn
          TopicArn: !Ref DataSyncTaskNotificationsTopic
      Code: 
        ZipFile: |
          import json
          from os import getenv
          from textwrap import dedent
          from boto3 import client
          from datetime import timedelta
          
          datasync_client = client("datasync")
          sns_client = client("sns")

          def lambda_handler(event, context):
              task_execution_arn = event['resources'][0]

              execution_status = datasync_client.describe_task_execution(
                  TaskExecutionArn=task_execution_arn
              )

              print(event)
              print(execution_status)

              should_publish_notification = execution_status["Status"] == "ERROR" or execution_status["BytesTransferred"] > 0

              if not should_publish_notification:
                print("No files were transferred, skipping notification")
                return None

              total_duration = execution_status["Result"]["TotalDuration"]
              total_duration_formatted = str(timedelta(milliseconds = total_duration))

              message = dedent(f"""
                  Dear Administrators,

                  The HALO DataSync task which started on {execution_status["StartTime"]} has completed execution with the following status: {execution_status["Status"]}.
                  For more information about individual files transferred, review the following CloudWatch log group: {getenv("LogGroupArn")}
                  
                  Task Name: {getenv("TaskName")}
                  Task ARN: {getenv("TaskArn")}
                  Task Execution ARN: {task_execution_arn}
                  Start Time: {execution_status["StartTime"]}
                  Total Duration (H:M:S): {total_duration_formatted}
                  Files Transferred: {execution_status["FilesTransferred"]}
                  GBs Transferred: {execution_status["BytesTransferred"] / 1e9}""")

              if execution_status["Status"] == "ERROR":
                  message += dedent(f"""
                      Error Code: {execution_status["Result"]["ErrorCode"]}
                      Error Details: {execution_status["Result"]["ErrorDetail"]}""")

              return sns_client.publish(
                  TopicArn=getenv("TopicArn"),
                  Subject=f"""DataSync Task ({getenv("TaskName")}) - {execution_status["Status"]}""",
                  Message=message
              )
    DependsOn: 
      - DataSyncTask
      - DataSyncTaskLogGroup
      - DataSyncTaskNotificationsRole
      - DataSyncTaskNotificationsTopic

  # Creates an EventBridge Rule which send DataSync task execution state change events to a Lambda function
  DataSyncTaskNotificationsEventRule:
    Type: "AWS::Events::Rule"
    Properties:
      Name: !Sub "${TaskName}DataSyncTaskNotifications"
      Description: !Sub "${TaskName}DataSyncTaskNotifications - Executes the specified Lambda function whenever the task's execution state changes"
      EventPattern:
        source: 
          - aws.datasync
        detail-type: 
          - DataSync Task Execution State Change
        resources:
          - prefix: !Sub "${DataSyncTask.TaskArn}/execution/"
        detail:
          State:
            - ERROR
            - SUCCESS
      Targets:
        - Id: !Ref DataSyncTaskNotificationsLambda
          Arn: !GetAtt DataSyncTaskNotificationsLambda.Arn
    DependsOn: 
      - DataSyncTask
      - DataSyncTaskNotificationsLambda
  
  # Grants permission for the DataSyncTaskNotificationsEventRule to invoke the DataSyncTaskNotificationsLambda function
  DataSyncTaskNotificationsLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      Action: "lambda:InvokeFunction"
      FunctionName: !GetAtt DataSyncTaskNotificationsLambda.Arn
      Principal: events.amazonaws.com
      SourceArn: !GetAtt DataSyncTaskNotificationsEventRule.Arn
    DependsOn:
      - DataSyncTaskNotificationsEventRule
      - DataSyncTaskNotificationsLambda

  #################################################################################################
  # Create a Lambda function which updates S3 object metadata for Storage Gateway SMB permissions #
  #################################################################################################

  # Creates a CloudWatch log group for a Lambda function which updates S3 object metadata for SMB permissions
  UpdateS3ObjectMetadataLogGroup:
    Type: "AWS::Logs::LogGroup"
    Properties: 
      LogGroupName: !Sub "/aws/lambda/${TaskName}UpdateS3ObjectMetadata"
      RetentionInDays: 180

  # Creates an IAM role for a Lambda function which modifies S3 object metadata for a given sync target
  UpdateS3ObjectMetadataRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal: 
              Service:
                - lambda.amazonaws.com
            Action:
               "sts:AssumeRole"
      Policies:
        - PolicyName: !Sub "${TaskName}UpdateS3ObjectMetadataPolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: 
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: !GetAtt UpdateS3ObjectMetadataLogGroup.Arn
              - Effect: Allow
                Action: 
                  - "logs:DescribeLogStreams"
                  - "logs:GetLogEvents"
                  - "logs:FilterLogEvents"
                Resource: !GetAtt DataSyncTaskLogGroup.Arn
              - Effect: Allow
                Action: 
                  - "datasync:DescribeTask"
                Resource: !GetAtt DataSyncTask.TaskArn
              - Effect: Allow
                Action: 
                  - "datasync:DescribeTaskExecution"
                Resource: !Sub "${DataSyncTask.TaskArn}/execution/*"
              - Effect: Allow
                Action: 
                  - "datasync:DescribeLocation*"
                Resource: 
                  - !Sub "arn:aws:datasync:${AWS::Region}:${AWS::AccountId}:location/${SourceLocationId}"
                  - !Sub "arn:aws:datasync:${AWS::Region}:${AWS::AccountId}:location/${DestinationLocationId}"
              - Effect: Allow
                Action:
                  - "sns:Publish"
                Resource: !Ref DataSyncTaskNotificationsTopic
              - Effect: Allow
                Action:
                  - "s3:DeleteObject*"
                  - "s3:GetObject*"
                  - "s3:PutObject*"
                Resource: 
                  - !Sub "arn:aws:s3:::${DestinationBucket}/*"

  # Creates a Lambda function which publishes updates about DataSync task execution status
  UpdateS3ObjectMetadataLambda:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: !Sub "${TaskName}UpdateS3ObjectMetadata"
      Description: !Sub "${TaskName} - Updates S3 object metadata for SMB permissions" 
      Handler: index.lambda_handler
      MemorySize: 4196
      Timeout: 900
      Runtime: python3.12
      Role: !GetAtt UpdateS3ObjectMetadataRole.Arn
      Environment:
        Variables:
          TZ: America/New_York
          TaskName: !Ref TaskName
          TaskArn: !GetAtt DataSyncTask.TaskArn
          LogGroupArn: !GetAtt DataSyncTaskLogGroup.Arn
          LogGroupName: !Ref DataSyncTaskLogGroup
          LambdaLogGroupArn: !GetAtt UpdateS3ObjectMetadataLogGroup.Arn
          TopicArn: !Ref DataSyncTaskNotificationsTopic
          SourceLocationArn: !Sub "arn:aws:datasync:${AWS::Region}:${AWS::AccountId}:location/${SourceLocationId}"
          DestinationLocationArn: !Sub "arn:aws:datasync:${AWS::Region}:${AWS::AccountId}:location/${DestinationLocationId}"
          DestinationBucket: !Ref DestinationBucket
          FileOwner: !Ref FileOwner
          FileGroup: !Ref FileGroup
          FilePermissions: !Ref FilePermissions
          FileAcl: !Ref FileAcl
      Code: 
        ZipFile: |
            import json
            from os import getenv, path
            from textwrap import dedent
            from boto3 import client
            from re import match, sub
            from traceback import print_exc
            from multiprocessing import Process
            from time import sleep
            from itertools import batched

            datasync_client = client("datasync")
            logs_client = client("logs")
            sns_client = client("sns")
            s3_client = client("s3")

            def get_synced_object_keys(execution_status):
                task_execution_arn = execution_status["TaskExecutionArn"]
                destination_bucket = getenv("DestinationBucket")
                _, task_id, _, execution_id = task_execution_arn.split("/")

                destination_location = datasync_client.describe_location_s3(
                    LocationArn=getenv("DestinationLocationArn")
                )

                key_prefix = destination_location["LocationUri"].replace(
                    f"s3://{destination_bucket}/", ""
                )

                paginator = logs_client.get_paginator('filter_log_events')
                response_iterator = paginator.paginate(
                    logGroupName=getenv("LogGroupName"),
                    logStreamNames=[f"{task_id}-{execution_id}"],
                    filterPattern='NOTICE Transferred file',
                )

                object_keys = []
                folder_keys = []
                
                for log_events in response_iterator:
                    for event in log_events["events"]:
                        matched_event = match("\[NOTICE\] Transferred file (.*), \d+ bytes", event['message'])
                        if matched_event is not None:
                            key = key_prefix + matched_event[1]
                            object_key = sub("/+", "/", key)
                            folder_key = path.dirname(object_key) + "/"

                            object_keys.append(object_key)
                            folder_keys.append(folder_key)

                return list(set(object_keys + folder_keys + [key_prefix]))


            def update_smb_metadata(bucket, key):
                s3_object_head = s3_client.head_object(Bucket=bucket, Key=key)
                s3_object_metadata = s3_object_head["Metadata"]
                s3_object_metadata_new = {}

                # Set object SMB permissions and associated gateway metadata
                s3_object_metadata_new["file-owner"] = getenv("FileOwner")
                s3_object_metadata_new["file-permissions"] = getenv("FilePermissions")
                s3_object_metadata_new["file-group"] = getenv("FileGroup")
                s3_object_metadata_new["file-acl"] = getenv("FileAcl")

                # Preserve metadata set by HALO
                if "file-ctime" in s3_object_metadata:
                    s3_object_metadata_new["file-ctime"] = s3_object_metadata["file-ctime"]
                if "file-mtime" in s3_object_metadata:
                    s3_object_metadata_new["file-mtime"] = s3_object_metadata["file-mtime"]

                return s3_client.copy_object(
                    Key=key,
                    Bucket=bucket,
                    CopySource={"Bucket": bucket, "Key": key},
                    Metadata=s3_object_metadata_new,
                    MetadataDirective="REPLACE",
                    StorageClass="INTELLIGENT_TIERING",
                )


            def lambda_handler(event, context):
                task_execution_arn = event["resources"][0]
                task_name = getenv("TaskName")
                bucket = getenv("DestinationBucket")
                print(task_execution_arn)
                sleep(60) # allow DataSync logs to settle

                execution_status = datasync_client.describe_task_execution(
                    TaskExecutionArn=task_execution_arn
                )

                try:
                    for batch_keys in batched(get_synced_object_keys(execution_status), 200):
                        processes = []
                        for key in batch_keys:
                            print(f"Update metadata: s3://{bucket}/{key}")
                            # update_smb_metadata(bucket, key) in parallel
                            p = Process(target=update_smb_metadata, args=[bucket, key])
                            p.start()
                            processes.append(p)
                        for p in processes:
                            p.join()
                except Exception as error:
                    print_exc()
                    sns_client.publish(
                        TopicArn=getenv("TopicArn"),
                        Subject=f"DataSync Task ({task_name}) - SMB PERMISSIONS ERROR",
                        Message=dedent(
                            f"""\
                            Dear Administrators,

                            The HALO DataSync task which started on {execution_status['StartTime']} failed to set SMB permissions on S3 object metadata. For more information, please review the following log group: {getenv("LambdaLogGroupArn")}"""
                        ),
                    )

  # Creates an EventBridge Rule which send DataSync task execution state change events to a Lambda function
  UpdateS3ObjectMetadataEventRule:
    Type: "AWS::Events::Rule"
    Properties:
      Name: !Sub "${TaskName}UpdateS3ObjectMetadata"
      Description: !Sub "${TaskName}UpdateS3ObjectMetadata - Executes the specified Lambda function whenever the task's execution state changes"
      EventPattern:
        source: 
          - aws.datasync
        detail-type: 
          - DataSync Task Execution State Change
        resources:
          - prefix: !Sub "${DataSyncTask.TaskArn}/execution/"
        detail:
          State:
            - SUCCESS
      Targets:
        - Id: !Ref UpdateS3ObjectMetadataLambda
          Arn: !GetAtt UpdateS3ObjectMetadataLambda.Arn

  # Grants permission to invoke the UpdateS3ObjectMetadataLambda function
  UpdateS3ObjectMetadataLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      Action: "lambda:InvokeFunction"
      FunctionName: !GetAtt UpdateS3ObjectMetadataLambda.Arn
      Principal: events.amazonaws.com
      SourceArn: !GetAtt UpdateS3ObjectMetadataEventRule.Arn
